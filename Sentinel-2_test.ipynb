{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\eo_data\\Ibague\n",
      "['data', 'S1_GRD', 'S2_L1C']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# change the working directory to the location of files\n",
    "os.chdir('D:/eo_data/Ibague/')\n",
    "print(os.getcwd())\n",
    "# store the files list to a variable\n",
    "eo_files = os.listdir(os.getcwd())\n",
    "print(eo_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip files\n",
    "import zipfile\n",
    "\n",
    "# Check if a data folder exist\n",
    "#if not os.path.exists('data'):\n",
    "#    os.makedirs('data')\n",
    "#    print 'data folder' + ' was created'\n",
    "# Check if the name of the data folder is in\n",
    "#if 'data' in eo_files:\n",
    "#    eo_files.remove('data')\n",
    "##TODO catch error when a directory is in eo_files\n",
    "\n",
    "def unzipfiles(path):\n",
    "    \"\"\"\n",
    "    Unzips every zipfile in the path, and stores in directory with zipfile name+.SAFE\n",
    "    Args:\n",
    "        path (string): string of directory where zipfiles are located \n",
    "    \"\"\"\n",
    "    for im_id in os.listdir(path):\n",
    "        if not os.path.exists('data/'+im_id[:-3]+'SAFE'):\n",
    "            print('Unzipping ' + im_id)\n",
    "            zip_ref = zipfile.ZipFile(path+im_id, 'r')\n",
    "            zip_ref.extractall('data')\n",
    "            zip_ref.close()\n",
    "        else:\n",
    "            print(im_id[:-4] + ' was already uncompressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2A_MSIL1C_20171021T153111_N0205_R025_T18NVL_20171021T153112 was already uncompressed\n",
      "S2A_MSIL1C_20171210T152631_N0206_R025_T18NVK_20171210T170838 was already uncompressed\n",
      "S2A_MSIL1C_20171220T153111_N0206_R025_T18NVK_20171220T170223 was already uncompressed\n",
      "S2A_MSIL1C_20171220T153111_N0206_R025_T18NVL_20171220T170223 was already uncompressed\n",
      "S2B_MSIL1C_20171016T153059_N0205_R025_T18NVK_20171016T153058 was already uncompressed\n",
      "S2B_MSIL1C_20171016T153059_N0205_R025_T18NVL_20171016T153058 was already uncompressed\n",
      "S2B_MSIL1C_20171105T153059_N0206_R025_T18NVK_20171105T170109 was already uncompressed\n",
      "S2B_MSIL1C_20171105T153059_N0206_R025_T18NVL_20171105T170109 was already uncompressed\n",
      "S2B_MSIL1C_20171205T152629_N0206_R025_T18NVK_20171205T170944 was already uncompressed\n",
      "S2B_MSIL1C_20171205T152629_N0206_R025_T18NVL_20171205T170944 was already uncompressed\n",
      "S2B_MSIL1C_20171215T152629_N0206_R025_T18NVK_20171215T201906 was already uncompressed\n",
      "S2B_MSIL1C_20171215T152629_N0206_R025_T18NVL_20171215T201906 was already uncompressed\n",
      "S2B_MSIL1C_20171225T152629_N0206_R025_T18NVL_20171225T171117 was already uncompressed\n",
      "S2B_MSIL1C_20180104T152629_N0206_R025_T18NVK_20180104T201829 was already uncompressed\n",
      "S2B_MSIL1C_20180104T152629_N0206_R025_T18NVL_20180104T201829 was already uncompressed\n",
      "S2B_MSIL1C_20180114T152629_N0206_R025_T18NVK_20180114T171011 was already uncompressed\n"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "unzipfiles(\"D:/eo_data/Ibague/S2_L1C/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process L1C to L2A images using sen2cor\n",
    "Call in console. To-do: check integration\n",
    "## Pre-process L2A images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import snappy\n",
    "import re\n",
    "\n",
    "from sentinelsat.sentinel import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "from snappy import ProductIO\n",
    "from snappy import HashMap\n",
    "import shutil\n",
    "import os  \n",
    "import ast\n",
    "\n",
    "from snappy import GPF\n",
    "\n",
    "GPF.getDefaultInstance().getOperatorSpiRegistry().loadOperatorSpis()\n",
    "HashMap = snappy.jpy.get_type('java.util.HashMap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S2A_MSIL2A_20171210T152631_N0206_R025_T18NVK_20171210T170838.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set location of products\n",
    "products_dir = \"D:/eo_data/Ibague/data/\"\n",
    "\n",
    "# get geoJSON file with region extent\n",
    "regPath = \"//dapadfs/workspace_cluster_6/TRANSVERSAL_PROJECTS/MADR/COMPONENTE_2/Imagenes_Satelitales/Sentinel_2/\" + \\\n",
    "                      \"JSON_Ibague/IbagueJSON.geojson\"\n",
    "geom = geojson_to_wkt(read_geojson(regPath))\n",
    "\n",
    "# get a list of S2 L2A products\n",
    "prdlist = filter(re.compile(r'^S2.....L2A').search, os.listdir(products_dir))\n",
    "\n",
    "## Create a dictionary to read Sentinel-1 L1 GRD products\n",
    "product = {}\n",
    "for element in prdlist:\n",
    "    product[element[:-4]] = {}\n",
    "\n",
    "\n",
    "product.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling...\n",
      "Subsetting...\n",
      "Writing...\n"
     ]
    }
   ],
   "source": [
    "## iterate over S2 L2A products\n",
    "    \n",
    "for key, value in product.iteritems():    \n",
    "    # Read the product\n",
    "    value['GRD'] = ProductIO.readProduct(products_dir+key+'SAFE/MTD_MSIL2A.xml')\n",
    "    \n",
    "    # Resample all bands to 10m resolution\n",
    "    resample_subset = HashMap()\n",
    "    resample_subset.put('targetResolution', 10)\n",
    "    print('Resampling...')\n",
    "    value['res10'] = GPF.createProduct(\"Resample\", resample_subset, value['GRD'])\n",
    "    \n",
    "    # Subset to area of interest\n",
    "    param_subset = HashMap()\n",
    "    param_subset.put('geoRegion', geom)\n",
    "    param_subset.put('outputImageScaleInDb', False)\n",
    "    param_subset.put('sourceBandNames', 'B2, B3, B4, B8, B11, B12, quality_cloud_confidence,quality_scene_classification')\n",
    "    print('Subsetting...')\n",
    "    value['sub'] = GPF.createProduct(\"Subset\", param_subset, value['res10'])\n",
    "    \n",
    "    # Write product\n",
    "    print('Writing...')\n",
    "    ProductIO.writeProduct(value['sub'], products_dir+'pre/'+key+'subset', 'BEAM-DIMAP')\n",
    "    \n",
    "    # Dispose all the intermediate products\n",
    "    value['GRD'].dispose()\n",
    "    value['res10'].dispose()\n",
    "    value['sub'].dispose()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading D:/eo_data/Ibague/data/test.data/B1.img\n",
      "Reading D:/eo_data/Ibague/data/prep/VH_specklefil_dB.dim\n",
      "Reading D:/eo_data/Ibague/data/prep/VV_specklefil_dB.dim\n",
      "Creating stack...\n",
      "Saving resulting product...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from snappy import ProductIO, GPF, jpy\n",
    "\n",
    "GPF.getDefaultInstance().getOperatorSpiRegistry().loadOperatorSpis()\n",
    "HashMap = jpy.get_type('java.util.HashMap')\n",
    "\n",
    "#collection of preprocessed files\n",
    "s1 = 'D:/eo_data/Ibague/data/prep/VH_specklefil_dB.dim'\n",
    "s2 = 'D:/eo_data/Ibague/data/prep/VV_specklefil_dB.dim'\n",
    "master = 'D:/eo_data/Ibague/data/test.data/B1.img'\n",
    "\n",
    "processed_files = [master,s1,s2]\n",
    "\n",
    "product_set=[]\n",
    "\n",
    "for f in processed_files:\n",
    "    product_set.append(ProductIO.readProduct(f))\n",
    "    print(\"Reading \"+f)\n",
    "      \n",
    "#define the stack parameters\n",
    "params = HashMap()\n",
    "params.put('resamplingType', 'NEAREST_NEIGHBOUR')\n",
    "params.put('initialOffsetMethod', 'Product Geolocation')\n",
    "params.put('extent', 'Master')\n",
    "\n",
    "#make the stack\n",
    "print(\"Creating stack...\")\n",
    "create_stack = GPF.createProduct('CreateStack', params, product_set)\n",
    "\n",
    "#write the stack\n",
    "print(\"Saving resulting product...\")\n",
    "ProductIO.writeProduct(create_stack, 'D:/eo_data/Ibague/data/create_stack', 'BEAM-DIMAP')\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
