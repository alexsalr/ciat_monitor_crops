{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel-1 data download using sentinelsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "from sentinelsat.sentinel import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "from datetime import date\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the environment variables and settings for downloads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Way to access the Windows server\n",
    "server_dir = \"/mnt/workspace_cluster_6/TRANSVERSAL_PROJECTS/MADR/COMPONENTE_2/Imagenes_Satelitales/Temp/asalazar_tests/\"\n",
    "\n",
    "# Get geoJSON file with region extent\n",
    "pathJSONFile=\"/mnt/workspace_cluster_6/TRANSVERSAL_PROJECTS/MADR/COMPONENTE_2/Imagenes_Satelitales/Sentinel_2/JSON_Ibague/IbagueJSON.geojson\"\n",
    "\n",
    "# Specify ESA scihub credentials\n",
    "scihub_user = 'asalazarr'\n",
    "scihub_pass = 'tila8sude'\n",
    "\n",
    "# os.chdir(server_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the call to the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "## TODO: remove hard coding of sen2cor installation\n",
    "def sen2cor_L2A (res, prod): # \n",
    "    \"\"\"\n",
    "    Function to call sen2cor L2A_Process for obtaining L2A products\n",
    "    \n",
    "    res (str/num): resolution, accepts 10, 20, 60, or all \n",
    "    prod (str): location of S1 L1C product\n",
    "    \"\"\"\n",
    "    # Hard-code location of sen2cor installation\n",
    "    os.chdir(\"/home/azalazar/DL_Temp/Sen2Cor-02.05.05-Linux64/bin\")\n",
    "    # Coerce resolution to string\n",
    "    res = str(res)\n",
    "    # Execute L2A_Process with resolution parameter when specified\n",
    "    if res == 'all':\n",
    "        os.system(\"bash L2A_Process\" + \" \" + prod)\n",
    "    else:\n",
    "        os.system(\"bash L2A_Process --resolution=\" + res + \" \" + str(prod))\n",
    "\n",
    "def sen2cor_L2A_batch (res, L1Cdir):\n",
    "    \"\"\"\n",
    "    Function to batch processing of S1-L1C files in a directory to S1-L2A\n",
    "    \n",
    "    res (str/num): resolution, accepts 10, 20, 60, or all\n",
    "    L1Cdir (str): location of S1 L1C products\n",
    "    \"\"\"\n",
    "    # Put S1 L1C directory names in list\n",
    "    L1C_files = filter(re.compile(r'^S2.....L1C').search, os.listdir(L1Cdir))\n",
    "    print(\"{} L1C files found in directory\".format(str(len(L1C_files))))\n",
    "    \n",
    "    for L1C_file in L1C_files: # Iterate over directory names\n",
    "        # Call sen2cor function for individual product\n",
    "        print(\"Processing {}\".format(L1C_file))\n",
    "        sen2cor_L2A(res, L1Cdir+L1C_file)\n",
    "        \n",
    "## Based on:     \n",
    "## Rodrigo Almeida, Maya Situnayake, Kees Baake, Mortimer Werther, Timon Weitkamp, Arnan Araza. Wageningen University\n",
    "## Academic Consultancy Project for EagleSensing. Remote Sensing and GIS Integration course, Period 6, 2016-2017.\n",
    "## https://github.com/rodrigoalmeida94/ACT_EagleSensing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing S2A_MSIL1C_20180320T152641_N0206_R025_T18NVK_20180320T184953.SAFE\n",
      "Processing S2A_MSIL1C_20180330T152641_N0206_R025_T18NVK_20180330T185707.SAFE\n",
      "Processing S2A_MSIL1C_20180330T152641_N0206_R025_T18NVL_20180330T185707.SAFE\n",
      "Processing S2B_MSIL1C_20180325T152639_N0206_R025_T18NVK_20180325T220529.SAFE\n",
      "Processing S2B_MSIL1C_20180325T152639_N0206_R025_T18NVL_20180325T220529.SAFE\n"
     ]
    }
   ],
   "source": [
    "sen2cor_L2A_batch('all', server_dir+'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "from sentinelsat.sentinel import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "from datetime import date\n",
    "import os\n",
    "\n",
    "def download_sentinel(platform, prod_type, scihub_user, scihub_pass, region, start_date, end_date, down_dir=None):\n",
    "    # change the working directory to the location of files\n",
    "    if down_dir!=None:\n",
    "        os.chdir(down_dir)\n",
    "    \n",
    "    # connect to the API\n",
    "    api = SentinelAPI(scihub_user, scihub_pass, 'https://scihub.copernicus.eu/dhus')\n",
    "    \n",
    "    # search by polygon, time, and Hub query keywords\n",
    "    ## TO-DO: add type of product to the search terms\n",
    "    footprint = geojson_to_wkt(read_geojson(region))\n",
    "    \n",
    "    products = api.query(footprint,\n",
    "                         date = (start_date, end_date),\n",
    "                         producttype = prod_type,\n",
    "                         platformname = platform)\n",
    "    \n",
    "    # download all results from the search\n",
    "    print(\"Files will be downloaded to {}\".format(os.getcwd()))\n",
    "    api.download_all(products)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='scihub.copernicus.eu', port=443): Max retries exceeded with url: /dhus/search?format=json&rows=100&start=0 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe48852ae90>: Failed to establish a new connection: [Errno 110] Connection timed out',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ff7e2ce57032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#download_sentinel('Sentinel-1', 'GRD', scihub_user, scihub_pass, pathJSONFile, '20180201', '20180228', server_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdownload_sentinel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentinel-2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'S2MSI1C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscihub_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscihub_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathJSONFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'20180301'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'20180331'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-606497e11f82>\u001b[0m in \u001b[0;36mdownload_sentinel\u001b[0;34m(platform, prod_type, scihub_user, scihub_pass, region, start_date, end_date, down_dir)\u001b[0m\n\u001b[1;32m     19\u001b[0m                          \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                          \u001b[0mproducttype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprod_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                          platformname = platform)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# download all results from the search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/azalazar/anaconda2/envs/snappy/lib/python2.7/site-packages/sentinelsat/sentinel.pyc\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, area, date, raw, area_relation, order_by, limit, offset, **keywords)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mformatted_order_by\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_order_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder_by\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatted_order_by\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSentinelAPIError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# Queries with length greater than about 2700-3600 characters (depending on content) may\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/azalazar/anaconda2/envs/snappy/lib/python2.7/site-packages/sentinelsat/sentinel.pyc\u001b[0m in \u001b[0;36m_load_query\u001b[0;34m(self, query, order_by, limit, offset)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_by\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mproducts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_subquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_by\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# repeat query until all results have been loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/azalazar/anaconda2/envs/snappy/lib/python2.7/site-packages/sentinelsat/sentinel.pyc\u001b[0m in \u001b[0;36m_load_subquery\u001b[0;34m(self, query, order_by, limit, offset)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# load query results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder_by\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0m_check_scihub_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/azalazar/anaconda2/envs/snappy/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \"\"\"\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'POST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/azalazar/anaconda2/envs/snappy/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    506\u001b[0m         }\n\u001b[1;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/azalazar/anaconda2/envs/snappy/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/azalazar/anaconda2/envs/snappy/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='scihub.copernicus.eu', port=443): Max retries exceeded with url: /dhus/search?format=json&rows=100&start=0 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe48852ae90>: Failed to establish a new connection: [Errno 110] Connection timed out',))"
     ]
    }
   ],
   "source": [
    "#download_sentinel('Sentinel-1', 'GRD', scihub_user, scihub_pass, pathJSONFile, '20180201', '20180228', server_dir)\n",
    "download_sentinel('Sentinel-2', 'S2MSI1C', scihub_user, scihub_pass, pathJSONFile, '20180301', '20180331', server_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip files\n",
    "import zipfile, re\n",
    "\n",
    "def unzipfiles(eo_dir, unzip_dir = 'data'):\n",
    "    \"\"\"\n",
    "    Unzips every zipfile in the path, and stores in directory with zipfile name+.SAFE\n",
    "    Args:\n",
    "        eo_dir (string): string of directory where zipfiles are located\n",
    "        unzip_dir (string): directory where files are to be unzipped, default relative path\n",
    "                            uz_data in working directory\n",
    "    \"\"\"\n",
    "    # List all zip files in directory\n",
    "    eo_files = filter(re.compile('zip$').search, os.listdir(eo_dir))\n",
    "    \n",
    "    # Check if a data folder exist\n",
    "    if not os.path.exists(unzip_dir):\n",
    "        os.makedirs(unzip_dir)\n",
    "        print unzip_dir + ' folder' + ' was created'\n",
    "    \n",
    "    # Make sure uzip direction ends with slash\n",
    "    if unzipdir[-1] != '/':\n",
    "        unzipdir = unzipdir + '/'\n",
    "    \n",
    "    ## Loop over list of zip files\n",
    "    for im_id in eo_files:\n",
    "        ## Unzip only if a folder with the same name does not exist\n",
    "        if not os.path.exists(unszip_dir+im_id[:-3]+'SAFE'):\n",
    "            print('Unzipping ' + im_id)\n",
    "            zip_ref = zipfile.ZipFile(im_id, 'r')\n",
    "            zip_ref.extractall('data')\n",
    "            zip_ref.close()\n",
    "        else:\n",
    "            print(im_id[:-4] + ' was already uncompressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data folder was created\n",
      "Unzipping S1B_IW_GRDH_1SDV_20170602T104209_20170602T104234_005870_00A4B1_774C.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20170614T104210_20170614T104235_006045_00A9D7_4B41.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20170626T104210_20170626T104235_006220_00AEED_0591.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20170708T104211_20170708T104236_006395_00B3E1_4D33.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20170720T104212_20170720T104237_006570_00B8E0_29C0.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20170801T104212_20170801T104237_006745_00BDE6_F318.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20170813T104213_20170813T104238_006920_00C2FF_99FD.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20170825T104213_20170825T104238_007095_00C80F_6556.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20170906T104214_20170906T104239_007270_00CD25_4F8F.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20170918T104214_20170918T104239_007445_00D249_A0FC.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20170930T104215_20170930T104240_007620_00D74F_27DD.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20171012T104215_20171012T104240_007795_00DC4C_C014.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20171024T104215_20171024T104240_007970_00E14C_9097.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20171105T104215_20171105T104240_008145_00E654_FADB.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20171117T104215_20171117T104240_008320_00EB98_BFA2.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20171129T104214_20171129T104239_008495_00F103_C116.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20171211T104214_20171211T104239_008670_00F691_F37A.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20171223T104214_20171223T104239_008845_00FC24_E700.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20180104T104213_20180104T104238_009020_0101D3_78E7.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20180116T104213_20180116T104238_009195_010785_5AA7.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20180128T104212_20180128T104237_009370_010D3E_6BBB.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20180305T104212_20180305T104237_009895_011E91_58B8.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20180317T104212_20180317T104237_010070_012455_9960.zip\n",
      "Unzipping S1B_IW_GRDH_1SDV_20180329T104212_20180329T104237_010245_0129FF_09FE.zip\n",
      "Unzipping S2A_MSIL1C_20171021T153111_N0205_R025_T18NVL_20171021T153112.zip\n",
      "Unzipping S2A_MSIL1C_20171210T152631_N0206_R025_T18NVK_20171210T170838.zip\n",
      "Unzipping S2A_MSIL1C_20171220T153111_N0206_R025_T18NVK_20171220T170223.zip\n",
      "Unzipping S2A_MSIL1C_20171220T153111_N0206_R025_T18NVL_20171220T170223.zip\n",
      "Unzipping S2A_MSIL1C_20180320T152641_N0206_R025_T18NVK_20180320T184953.zip\n",
      "Unzipping S2A_MSIL1C_20180330T152641_N0206_R025_T18NVK_20180330T185707.zip\n",
      "Unzipping S2A_MSIL1C_20180330T152641_N0206_R025_T18NVL_20180330T185707.zip\n",
      "Unzipping S2B_MSIL1C_20171016T153059_N0205_R025_T18NVK_20171016T153058.zip\n",
      "Unzipping S2B_MSIL1C_20171016T153059_N0205_R025_T18NVL_20171016T153058.zip\n",
      "Unzipping S2B_MSIL1C_20171105T153059_N0206_R025_T18NVK_20171105T170109.zip\n",
      "Unzipping S2B_MSIL1C_20171105T153059_N0206_R025_T18NVL_20171105T170109.zip\n",
      "Unzipping S2B_MSIL1C_20171205T152629_N0206_R025_T18NVK_20171205T170944.zip\n",
      "Unzipping S2B_MSIL1C_20171205T152629_N0206_R025_T18NVL_20171205T170944.zip\n",
      "Unzipping S2B_MSIL1C_20171215T152629_N0206_R025_T18NVK_20171215T201906.zip\n",
      "Unzipping S2B_MSIL1C_20171215T152629_N0206_R025_T18NVL_20171215T201906.zip\n",
      "Unzipping S2B_MSIL1C_20171225T152629_N0206_R025_T18NVL_20171225T171117.zip\n",
      "Unzipping S2B_MSIL1C_20180104T152629_N0206_R025_T18NVK_20180104T201829.zip\n",
      "Unzipping S2B_MSIL1C_20180104T152629_N0206_R025_T18NVL_20180104T201829.zip\n",
      "Unzipping S2B_MSIL1C_20180114T152629_N0206_R025_T18NVK_20180114T171011.zip\n",
      "Unzipping S2B_MSIL1C_20180325T152639_N0206_R025_T18NVK_20180325T220529.zip\n",
      "Unzipping S2B_MSIL1C_20180325T152639_N0206_R025_T18NVL_20180325T220529.zip\n",
      "S2B_MSIL1C_20180325T152639_N0206_R025_T18NVL_20180325T220529 was already uncompressed\n"
     ]
    }
   ],
   "source": [
    "unzip_eofiles(server_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing using SNAP\n",
    "The pre-processing workflow (to-be revised) is performed using SNAP Python API, snappy, and currently incudes the following steps:\n",
    "1. Apply orbit\n",
    "2. Speckle filtering\n",
    "3. Terrain correction\n",
    "4. Subset the area of interest\n",
    "5. Logaritmic transformation (to dB)\n",
    "6. Texture analysis\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snappy\n",
    "\n",
    "from sentinelsat.sentinel import read_geojson, geojson_to_wkt\n",
    "from snappy import ProductIO\n",
    "from snappy import HashMap\n",
    "import shutil\n",
    "import os  \n",
    "import ast\n",
    "\n",
    "from snappy import GPF\n",
    "\n",
    "GPF.getDefaultInstance().getOperatorSpiRegistry().loadOperatorSpis()\n",
    "HashMap = snappy.jpy.get_type('java.util.HashMap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Op name: org.esa.s1tbx.insar.gpf.coregistration.CreateStackOp\n",
      "Op alias: CreateStack\n",
      "\n",
      "PARAMETERS:\n",
      "\n",
      "masterBandNames: The list of source bands.\n",
      "Default Value: None\n",
      "\n",
      "slaveBandNames: The list of source bands.\n",
      "Default Value: None\n",
      "\n",
      "resamplingType: The method to be used when resampling the slave grid onto the master grid.\n",
      "Default Value: NONE\n",
      "\n",
      "extent: The output image extents.\n",
      "Default Value: Master\n",
      "\n",
      "initialOffsetMethod: Method to be used in computation of initial offset between master and slave\n",
      "Default Value: Orbit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Function to get help of the SNAP operators\n",
    "\n",
    "def Op_help(op):\n",
    "        op_spi = snappy.GPF.getDefaultInstance().getOperatorSpiRegistry().getOperatorSpi(op)\n",
    "        print('Op name: {}'.format(op_spi.getOperatorDescriptor().getName()))\n",
    "        print('Op alias: {}\\n'.format(op_spi.getOperatorDescriptor().getAlias()))\n",
    "        print('PARAMETERS:\\n')\n",
    "        param_Desc = op_spi.getOperatorDescriptor().getParameterDescriptors()\n",
    "        for param in param_Desc:\n",
    "            value_set = param_Desc[0].getValueSet()\n",
    "            if len(value_set) == 0:\n",
    "                print('{}: {}\\nDefault Value: {}\\n'.format(param.getName(),param.getDescription(),param.getDefaultValue()))\n",
    "            else:\n",
    "                print('{}: {}\\nDefault Value: {}\\nPossible param: {}\\n'.format(param.getName(),param.getDescription(),param.getDefaultValue(),list(value_set)))\n",
    "\n",
    "#Op_help(\"Multi-Temporal-Speckle-Filter\")\n",
    "Op_help(\"CreateStack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a list of all Sentinel-1 toolbox operators\n",
    "op_spi = snappy.GPF.getDefaultInstance().getOperatorSpiRegistry().getOperatorSpis().toString()\n",
    "op_list = op_spi.split(', ')\n",
    "listname = []\n",
    "for op_str in op_list:\n",
    "    to_add = op_str\n",
    "    if op_str[0] == '[': to_add = op_str[1:]\n",
    "    elif op_str[-1] == ']': to_add = op_str[:-1]\n",
    "    #if to_add.split('.')[2] == 's1tbx':\n",
    "    listname.append(to_add.split('$')[0])\n",
    "listname.sort()\n",
    "#for name in listname:\n",
    "#    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and process S1 products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reads directories in a folder containing only S1 products. Then, creates a dictionary using the products names as keys. Then, reads and stores diferent processing steps in a second-level dictionary (i.e. inside the first dictionary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: 'smb://dapadfs/workspace_cluster_6/TRANSVERSAL_PROJECTS/MADR/COMPONENTE_2/Imagenes_Satelitales/Sentinel_2/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-058f9fecd659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Get the names of the downloaded files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0meo_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meo_direc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meo_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: 'smb://dapadfs/workspace_cluster_6/TRANSVERSAL_PROJECTS/MADR/COMPONENTE_2/Imagenes_Satelitales/Sentinel_2/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set a variable for the location of the data\n",
    "## To Do. Check how to access server\n",
    "\n",
    "eo_direc = r\"smb://dapadfs/workspace_cluster_6/TRANSVERSAL_PROJECTS/MADR/COMPONENTE_2/Imagenes_Satelitales/Sentinel_2/\"\n",
    "\n",
    "# Get the names of the downloaded files\n",
    "eo_files = os.listdir(eo_direc)\n",
    "list(eo_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/azalazar/GitHub/ciat_monitor_crops\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dictionary to read Sentinel-1 L1 GRD products\n",
    "product = {}\n",
    "for element in eo_files:\n",
    "    product[element[:-5]] = {}\n",
    "\n",
    "# Define the area of interest\n",
    "WKTReader = snappy.jpy.get_type('com.vividsolutions.jts.io.WKTReader')\n",
    "regPath = \"//dapadfs/workspace_cluster_6/TRANSVERSAL_PROJECTS/MADR/COMPONENTE_2/Imagenes_Satelitales/Sentinel_2/\" + \\\n",
    "                      JSON_Saldana/saldanaextent.geojson\"\n",
    "geom = geojson_to_wkt(read_geojson(regPath))\n",
    "\n",
    "# Define polarizations of interest\n",
    "polarizations = ['VV', 'VH']\n",
    "\n",
    "# Final results dictionary, with lists\n",
    "results = []\n",
    "#for pol in polarizations:\n",
    "#    results[pol] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S1A_IW_GRDH_1SSV_20151017T104253',\n",
       " 'S1A_IW_GRDH_1SSV_20160419T231329',\n",
       " 'S1A_IW_GRDH_1SSV_20150612T231317',\n",
       " 'S1A_IW_GRDH_1SDV_20150916T231330',\n",
       " 'S1A_IW_GRDH_1SSV_20150806T104250',\n",
       " 'S1A_IW_GRDH_1SSV_20160309T104258',\n",
       " 'S1A_IW_GRDH_1SSV_20151221T231315',\n",
       " 'S1A_IW_GRDH_1SSV_20150823T231320',\n",
       " 'S1A_IW_GRDH_1SSV_20160121T104259',\n",
       " 'S1A_IW_GRDH_1SSV_20160207T231327',\n",
       " 'S1A_IW_GRDH_1SSV_20160214T104258',\n",
       " 'S1A_IW_GRDH_1SSV_20151204T104300',\n",
       " 'S1A_IW_GRDH_1SSV_20150619T104249',\n",
       " 'S1A_IW_GRDH_1SSV_20150730T231319',\n",
       " 'S1A_IW_GRDH_1SSV_20151110T104241',\n",
       " 'S1A_IW_GRDH_1SSV_20151010T231322',\n",
       " 'S1A_IW_GRDH_1SSV_20160402T104259',\n",
       " 'S1A_IW_GRDH_1SSV_20150830T104252',\n",
       " 'S1A_IW_GRDH_1SSV_20151103T231321',\n",
       " 'S1A_IW_GRDH_1SDV_20151127T231327',\n",
       " 'S1A_IW_GRDH_1SSV_20160302T231327',\n",
       " 'S1A_IW_GRDH_1SSV_20160114T231328',\n",
       " 'S1A_IW_GRDH_1SSV_20150713T104249']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading S1A_IW_GRDH_1SSV_20151017T104253\n",
      "Reading S1A_IW_GRDH_1SSV_20160419T231329\n",
      "Reading S1A_IW_GRDH_1SSV_20150612T231317\n",
      "Reading S1A_IW_GRDH_1SDV_20150916T231330\n",
      "Reading S1A_IW_GRDH_1SSV_20150806T104250\n",
      "Reading S1A_IW_GRDH_1SSV_20160309T104258\n",
      "Reading S1A_IW_GRDH_1SSV_20151221T231315\n",
      "Reading S1A_IW_GRDH_1SSV_20150823T231320\n",
      "Reading S1A_IW_GRDH_1SSV_20160121T104259\n",
      "Reading S1A_IW_GRDH_1SSV_20160207T231327\n",
      "Reading S1A_IW_GRDH_1SSV_20160214T104258\n",
      "Reading S1A_IW_GRDH_1SSV_20151204T104300\n",
      "Reading S1A_IW_GRDH_1SSV_20150619T104249\n",
      "Reading S1A_IW_GRDH_1SSV_20150730T231319\n",
      "Reading S1A_IW_GRDH_1SSV_20151110T104241\n",
      "Reading S1A_IW_GRDH_1SSV_20151010T231322\n",
      "Reading S1A_IW_GRDH_1SSV_20160402T104259\n",
      "Reading S1A_IW_GRDH_1SSV_20150830T104252\n",
      "Reading S1A_IW_GRDH_1SSV_20151103T231321\n",
      "Reading S1A_IW_GRDH_1SDV_20151127T231327\n",
      "Reading S1A_IW_GRDH_1SSV_20160302T231327\n",
      "Reading S1A_IW_GRDH_1SSV_20160114T231328\n",
      "Reading S1A_IW_GRDH_1SSV_20150713T104249\n"
     ]
    }
   ],
   "source": [
    "for key, value in product.iteritems():    \n",
    "    # Read the product\n",
    "    value['GRD'] = ProductIO.readProduct(eo_direc+key+'.SAFE/manifest.safe')\n",
    "    print('Reading '+key)\n",
    "    \n",
    "    # Apply orbit\n",
    "    param_orbit = HashMap()\n",
    "    value['orbit'] = GPF.createProduct(\"Apply-Orbit-File\", param_orbit, value['GRD'])\n",
    "    \n",
    "    # The following operations are specific por each polarization    \n",
    "    for pol in polarizations:\n",
    "        # Radiometric calibration\n",
    "        param_calibration = HashMap()\n",
    "        param_calibration.put('outputSigmaBand', True)\n",
    "        param_calibration.put('sourceBands', getBandNames(value['orbit'], 'Intensity_'+pol))\n",
    "        param_calibration.put('selectedPolarisations', pol)\n",
    "        param_calibration.put('outputImageScaleInDb', False)\n",
    "        value['calibration_'+pol] = GPF.createProduct(\"Calibration\", param_calibration, value['orbit'])\n",
    "        \n",
    "        # Terrain correction\n",
    "        param_terraincor = HashMap()\n",
    "        param_terraincor.put('demResamplingMethod', 'NEAREST_NEIGHBOUR')\n",
    "        param_terraincor.put('imgResamplingMethod', 'NEAREST_NEIGHBOUR')\n",
    "        param_terraincor.put('applyRadiometricNormalization', True)\n",
    "        param_terraincor.put('demName', 'SRTM 3Sec')\n",
    "        param_terraincor.put('pixelSpacingInMeter', 10.0)\n",
    "        param_terraincor.put('sourceBands', getBandNames(value['calibration_'+pol], 'Sigma0_'+pol))\n",
    "        param_terraincor.put('mapProjection', 'WGS84(DD)')\n",
    "        value['terraincor_'+pol] = GPF.createProduct(\"Terrain-Correction\", param_terraincor, value['calibration_'+pol])\n",
    "        \n",
    "        # Subset to area of interest\n",
    "        param_subset = HashMap()\n",
    "        param_subset.put('geoRegion', geom)\n",
    "        param_subset.put('outputImageScaleInDb', False)\n",
    "        param_subset.put('sourceBandNames', getBandNames(value['terraincor_'+pol], 'Sigma0_'+pol))\n",
    "        value['subset_'+pol] = GPF.createProduct(\"Subset\", param_subset, value['terraincor_'+pol])\n",
    "        \n",
    "        # define the name of the output\n",
    "        #output_name = eo_direc + 'prep/' + key + \"_\" + pol + \"_\"\n",
    "        \n",
    "        # Write the results to files\n",
    "        #ProductIO.writeProduct(value['subset_'+pol], output_name, 'BEAM-DIMAP')\n",
    "        results.append(value['subset_'+pol])#product\n",
    "        \n",
    "        # dispose all the intermediate products\n",
    "        #value['calibration_'+pol].dispose()\n",
    "        #value['terraincor_'+pol].dispose()\n",
    "        #value['subset_'+pol].dispose()\n",
    "        \n",
    "    #dispose all the intermediate products\n",
    "    #value['GRD'].dispose()\n",
    "    #value['orbit'].dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating stack...\n"
     ]
    }
   ],
   "source": [
    "stack = stacking(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-temporal speckle VV\n",
      "Sigma0_VV_mst_14Jan2016,Sigma0_VV_slv1_02Mar2016,Sigma0_VV_slv3_13Jul2015,Sigma0_VV_slv4_10Oct2015,Sigma0_VV_slv5_30Aug2015,Sigma0_VV_slv6_02Apr2016,Sigma0_VV_slv7_27Nov2015,Sigma0_VV_slv8_03Nov2015,Sigma0_VV_slv9_09Mar2016,Sigma0_VV_slv10_23Aug2015,Sigma0_VV_slv11_21Dec2015,Sigma0_VV_slv12_14Feb2016,Sigma0_VV_slv13_19Jun2015,Sigma0_VV_slv14_04Dec2015,Sigma0_VV_slv15_10Nov2015,Sigma0_VV_slv16_30Jul2015,Sigma0_VV_slv18_06Aug2015,Sigma0_VV_slv19_16Sep2015,Sigma0_VV_slv20_12Jun2015,Sigma0_VV_slv21_21Jan2016,Sigma0_VV_slv22_07Feb2016,Sigma0_VV_slv23_19Apr2016,Sigma0_VV_slv24_17Oct2015\n",
      "Multi-temporal speckle VH\n",
      "Sigma0_VH_slv2_27Nov2015,Sigma0_VH_slv17_16Sep2015\n"
     ]
    }
   ],
   "source": [
    "print('Multi-temporal speckle VV')\n",
    "speckle_VV = mtspeckle_sigma0(stack, 'VV')\n",
    "print(getBandNames(speckle_VV))\n",
    "\n",
    "print('Multi-temporal speckle VH')\n",
    "speckle_VH = mtspeckle_sigma0(stack, 'VH')\n",
    "print(getBandNames(speckle_VH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing VH\n",
      "Writing VV\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "java.lang.NullPointerException",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-6c4519e003ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mProductIO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteProduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSigma0_todB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeckle_VH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meo_direc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'VH_specklefil_dB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BEAM-DIMAP'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Writing VV'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mProductIO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteProduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSigma0_todB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeckle_VV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meo_direc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'VV_specklefil_dB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BEAM-DIMAP'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: java.lang.NullPointerException"
     ]
    }
   ],
   "source": [
    "print('Writing VH')\n",
    "ProductIO.writeProduct(Sigma0_todB(speckle_VH), eo_direc + 'VH_specklefil_dB', 'BEAM-DIMAP')\n",
    "print('Writing VV')\n",
    "ProductIO.writeProduct(Sigma0_todB(speckle_VV), eo_direc + 'VV_specklefil_dB', 'BEAM-DIMAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in product.iteritems():\n",
    "    for pol in polarizations:\n",
    "        value['calibration_'+pol].dispose()\n",
    "        value['terraincor_'+pol].dispose()\n",
    "        value['subset_'+pol].dispose()\n",
    "    value['GRD'].dispose()\n",
    "    value['orbit'].dispose()\n",
    "stack.dispose()\n",
    "speckle_VV.dispose()\n",
    "speckle_VH.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speckle filtering \n",
    "\n",
    "* Multi-temporal speckle filtering\n",
    "* Scale transformation to dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read all the VV/VH files and store in a list (for further pre-processing)\n",
    "import os, re\n",
    "\n",
    "filenames_VV = filter(re.compile(r'VV_\\.dim').search, os.listdir(os.getcwd()+'\\data\\prep'))\n",
    "filenames_VH = filter(re.compile(r'VH_\\.dim').search, os.listdir(os.getcwd()+'\\data\\prep'))\n",
    "\n",
    "VV_l = []\n",
    "for eofile in pre_VV:\n",
    "    VV_l.append(ProductIO.readProduct(os.getcwd()+'/data/prep/' + eofile))\n",
    "\n",
    "VH_l = []\n",
    "for eofile in pre_VH:\n",
    "    VH_l.append(ProductIO.readProduct(os.getcwd()+'/data/prep/' + eofile))\n",
    "    \n",
    "## Apply multi-temporal speckle filtering\n",
    "sf_product_VV = mtspeckle_sigma0(VV_stack)\n",
    "sf_product_VH = mtspeckle_sigma0(VH_stack)\n",
    "\n",
    "# Change to logaritmic units (dB) \n",
    "db_product_VV = Sigma0_todB(sf_product_VV)\n",
    "db_product_VH = Sigma0_todB(sf_product_VH)\n",
    "\n",
    "## Write the results\n",
    "ProductIO.writeProduct(db_product_VV, eo_direc + 'prep/VH_specklefil_dB', 'BEAM-DIMAP')\n",
    "ProductIO.writeProduct(db_product_VH, eo_direc + 'prep/VH_specklefil_dB', 'BEAM-DIMAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to apply multi-temporal speckle filter of a stacked product Sigma0 bands\n",
    "\"\"\"\n",
    "Multi-Temporal-Speckle-Filter Op Parameters:\n",
    "    filter: None --- Default Value: Lee Sigma\n",
    "    nfilterSizeX: The kernel x dimension --- Default Value: 3\n",
    "    nfilterSizeY: The kernel y dimension --- Default Value: 3\n",
    "    ndampingFactor: The damping factor (Frost filter only) --- Default Value: 2\n",
    "    nestimateENL: None --- Default Value: false\n",
    "    nenl: The number of looks --- Default Value: 1.0\n",
    "    nnumLooksStr: None --- nDefault Value: 1\n",
    "    nwindowSize: None --- Default Value: 7x7\n",
    "    ntargetWindowSizeStr: None --- Default Value: 3x3\n",
    "    nsigmaStr: None --- Default Value: 0.9\n",
    "    nanSize: The Adaptive Neighbourhood size --- Default Value: 50\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def getBandNames (product, sfilter = ''):\n",
    "    \"\"\"\n",
    "    Produces a string to use in the sourceBandNames parameter specification of SNAP operators.\n",
    "    Args:\n",
    "        product (): \n",
    "        sfilter (string): regular expression to filter the name of the bands\n",
    "    Output:\n",
    "        returns a string with comma-separated band names\n",
    "    \"\"\"\n",
    "    band_names = product.getBandNames()\n",
    "    if sfilter != '':\n",
    "        band_names = filter(re.compile(r''+sfilter).search, band_names)\n",
    "    if len(band_names) > 0:\n",
    "        band_names = ','.join(band_names)\n",
    "    else:\n",
    "        band_names = None\n",
    "    return band_names\n",
    "\n",
    "def mtspeckle_sigma0 (stacked_prod, pol):\n",
    "    \"\"\"\n",
    "    Applies the a multi-temporal speckle filter to the a corregistered calibrated product stack. Takes the product bands\n",
    "    which name starts with 'Sigma0'.\n",
    "    \n",
    "    Args:\n",
    "        stacked_prod (): product with all the bands to be used for the multi-temporal speckle filter operation\n",
    "        pol (str): polarization to apply the speckle filter (VV or VH)\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    param_specklefilter = HashMap()\n",
    "    param_specklefilter.put('sourceBandNames', getBandNames(stacked_prod, \"Sigma0_\"+pol))\n",
    "    param_specklefilter.put('filter', 'Lee Sigma')\n",
    "    sf_product = GPF.createProduct(\"Multi-Temporal-Speckle-Filter\", param_specklefilter, stacked_prod)\n",
    "    return sf_product\n",
    "\n",
    "def Sigma0_todB (product):\n",
    "    \"\"\"\n",
    "    Transforms the product bands to a logaritmic scale in dB (10*log10[band]).\n",
    "    \n",
    "    Args:\n",
    "        product: product with Sigma0 bands in linear units\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    param_logdB = HashMap()\n",
    "    param_logdB.put('sourceBandNames', getBandNames(product))\n",
    "    db_product = GPF.createProduct(\"LinearToFromdB\", param_logdB, product)\n",
    "    return db_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that iterates over the list of products and stacks the its bands using the SNAP Collocate Operation\n",
    "\n",
    "def stacking(product_set):\n",
    "    # define the stack parameters\n",
    "    params = HashMap()\n",
    "    params.put('resamplingType', None)\n",
    "    params.put('initialOffsetMethod', 'Product Geolocation')\n",
    "    params.put('extent', 'Master')\n",
    "    \n",
    "    # create the stack\n",
    "    print(\"Creating stack...\")\n",
    "    create_stack = GPF.createProduct('CreateStack', params, product_set)\n",
    "    \n",
    "    return create_stack\n",
    "\n",
    "\n",
    "def stacking_ (prod_list):\n",
    "    \"\"\"\n",
    "    Recursive function. Takes a list of SNAP products and returns a stacked product with all the bands named with\n",
    "    the products acquisition dates.\n",
    "    Args:\n",
    "        prod_list: a list of products to be stacked\n",
    "    Output: returns an individual product with the bands of the other products \n",
    "    \"\"\"\n",
    "    if len(prod_list) == 1:\n",
    "        return prod_list[-1]\n",
    "    else:\n",
    "        param_coll = HashMap()\n",
    "        input_coll = HashMap()\n",
    "        input_coll.put(\"master\", prod_list[0])\n",
    "        input_coll.put(\"slave\", prod_list[1])\n",
    "        param_coll.put(\"masterComponentPattern\",\"${ORIGINAL_NAME}_\"+prod_list[0].getName()[17:31])\n",
    "        param_coll.put(\"slaveComponentPattern\",\"${ORIGINAL_NAME}_\"+prod_list[1].getName()[17:31])\n",
    "        prod_list[1] = GPF.createProduct('Collocate', param_coll, input_coll)\n",
    "        return stacking_(prod_list[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texture analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# paramaters for GLCM texture analysis\n",
    "paramGLCM = HashMap()\n",
    "paramGLCM.put('sourceBandNames', 'Sigma0_' + 'VV')\n",
    "paramGLCM.put('windowSizeStr', '5x5')\n",
    "paramGLCM.put('quantizerStr', 'Probabilistic Quantizer')\n",
    "paramGLCM.put('quantizationLevelsStr', '16')\n",
    "paramGLCM.put('displacement','4' )\n",
    "paramGLCM.put('outputContrast','true')\n",
    "paramGLCM.put('outputDissimilarity','true')\n",
    "paramGLCM.put('outputHomogeneity','true')\n",
    "paramGLCM.put('outputASM','true')\n",
    "paramGLCM.put('outputEnergy','true')\n",
    "paramGLCM.put('outputMean','true')\n",
    "paramGLCM.put('outputVariance','true')\n",
    "paramGLCM.put('outputCorrelation','true')\n",
    "\n",
    "product = GPF.createProduct(\"GLCM\", paramGLCM, product)\n",
    "\n",
    "#GLCM_output = \"C:/Users/asalazar/Documents/Ibague/_subset_VV\"\n",
    "\n",
    "#ProductIO.writeProduct(targetGCLM, GLCM_output, 'BEAM-DIMAP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor function. Could be useful for monitoring progress of file writing. To test!\n",
    "\n",
    "from snappy import jpy\n",
    "\n",
    "def createProgressMonitor():\n",
    "    PWPM = jpy.get_type('com.bc.ceres.core.PrintWriterProgressMonitor')\n",
    "    JavaSystem = jpy.get_type('java.lang.System')\n",
    "    monitor = PWPM(JavaSystem.out)\n",
    "    return monitor\n",
    "\n",
    "ProductIO.writeProduct(product, 'C:/Users/asalazar/Documents/Ibague/GLCM_output', 'BEAM-DIMAP', pm = createProgressMonitor())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
