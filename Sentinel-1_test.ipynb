{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel-1 data download using sentinelsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "from sentinelsat.sentinel import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "from datetime import date\n",
    "import os\n",
    "\n",
    "def download_sentinel(platform, prod_type, scihub_user, scihub_pass, region, start_date, end_date, down_dir=None):\n",
    "    # change the working directory to the location of files\n",
    "    if down_dir!=None:\n",
    "        os.chdir(down_dir)\n",
    "    \n",
    "    # connect to the API\n",
    "    api = SentinelAPI(scihub_user, scihub_pass, 'https://tmphub.copernicus.eu/dhus/')#'https://scihub.copernicus.eu/dhus')\n",
    "    \n",
    "    # search by polygon, time, and Hub query keywords\n",
    "    ## TO-DO: add type of product to the search terms\n",
    "    footprint = geojson_to_wkt(read_geojson(region))\n",
    "    \n",
    "    products = api.query(footprint,\n",
    "                         date = (start_date, end_date),\n",
    "                         producttype = prod_type,\n",
    "                         platformname = platform)\n",
    "    \n",
    "    # download all results from the search\n",
    "    print(\"Files will be downloaded to {}\".format(os.getcwd()))\n",
    "    api.download_all(products)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the environment variables and settings for downloads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GeoJSON file with region extent\n",
    "Ibague_ext =\"/mnt/workspace_cluster_6/TRANSVERSAL_PROJECTS/MADR/COMPONENTE_2/Imagenes_Satelitales/Sentinel_2/JSON_Ibague/IbagueJSON.geojson\"\n",
    "# Set download directory\n",
    "downdir = '/home/azalazar/data'\n",
    "# Set ESA scihub credentials\n",
    "scihub_user = 'asalazarr'\n",
    "scihub_pass = 'tila8sude'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download of Sentinel-1 and Sentinel-2 from march 2018 images for Ibague region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SentinelAPIError",
     "evalue": "HTTP status 401 Unauthorized: \n# HTTP Status 401 - Full authentication is required to access this resource\n\n **type** Status report\n\n **message** _Full authentication is required to access this resource_\n\n **description** _This request requires HTTP authentication._\n\n* * *\n\n### Apache Tomcat/8.0.36",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSentinelAPIError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c75cc1a07cbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#download_sentinel('Sentinel-1', 'GRD', scihub_user, scihub_pass, Ibague_ext, \"20180328\", \"20180331\", downdir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdownload_sentinel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentinel-2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'S2MSI1C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscihub_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscihub_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIbague_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"20180328\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"20180331\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowndir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-0c5e56215d2a>\u001b[0m in \u001b[0;36mdownload_sentinel\u001b[0;34m(platform, prod_type, scihub_user, scihub_pass, region, start_date, end_date, down_dir)\u001b[0m\n\u001b[1;32m     19\u001b[0m                          \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                          \u001b[0mproducttype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprod_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                          platformname = platform)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# download all results from the search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/azalazar/anaconda2/envs/snappy/lib/python2.7/site-packages/sentinelsat/sentinel.pyc\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, area, date, raw, area_relation, order_by, limit, offset, **keywords)\u001b[0m\n\u001b[1;32m    142\u001b[0m                          \"({:.1%} of the limit)\".format(factor))\n\u001b[1;32m    143\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found %s products\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_parse_opensearch_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSentinelAPIError\u001b[0m: HTTP status 401 Unauthorized: \n# HTTP Status 401 - Full authentication is required to access this resource\n\n **type** Status report\n\n **message** _Full authentication is required to access this resource_\n\n **description** _This request requires HTTP authentication._\n\n* * *\n\n### Apache Tomcat/8.0.36"
     ]
    }
   ],
   "source": [
    "#download_sentinel('Sentinel-1', 'GRD', scihub_user, scihub_pass, Ibague_ext, \"20180328\", \"20180331\", downdir)\n",
    "download_sentinel('Sentinel-2', 'S2MSI1C', scihub_user, scihub_pass, Ibague_ext, \"20180328\", \"20180331\", downdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip files\n",
    "import zipfile\n",
    "\n",
    "def unzip_eofiles(eo_dir, unzip_dir = 'uz_data'):\n",
    "    # List all zip files in directory\n",
    "    eo_files = filter(re.compile('zip$').search, os.listdir(eo_dir))\n",
    "    \n",
    "    # Check if a data folder exist\n",
    "    if not os.path.exists(unzip_dir):\n",
    "        os.makedirs(unzip_dir)\n",
    "        print 'data folder' + ' was created'\n",
    "    \n",
    "    ## Loop over list of zip files\n",
    "    for im_id in eo_files:\n",
    "        if not os.path.exists('data/'+im_id[:-3]+'SAFE'):\n",
    "            print('Unzipping ' + im_id)\n",
    "        zip_ref = zipfile.ZipFile(im_id, 'r')\n",
    "        zip_ref.extractall('data')\n",
    "        zip_ref.close()\n",
    "    else:\n",
    "        print(im_id[:-4] + ' was already uncompressed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing using SNAP\n",
    "The pre-processing workflow (to-be revised) is performed using SNAP Python API, snappy, and currently incudes the following steps:\n",
    "1. Apply orbit\n",
    "2. Speckle filtering\n",
    "3. Terrain correction\n",
    "4. Subset the area of interest\n",
    "5. Logaritmic transformation (to dB)\n",
    "6. Texture analysis\n",
    "\n",
    "***\n",
    "Help functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a list of all Sentinel-1 toolbox operators\n",
    "op_spi = snappy.GPF.getDefaultInstance().getOperatorSpiRegistry().getOperatorSpis().toString()\n",
    "op_list = op_spi.split(', ')\n",
    "listname = []\n",
    "for op_str in op_list:\n",
    "    to_add = op_str\n",
    "    if op_str[0] == '[': to_add = op_str[1:]\n",
    "    elif op_str[-1] == ']': to_add = op_str[:-1]\n",
    "    #if to_add.split('.')[2] == 's1tbx':\n",
    "    listname.append(to_add.split('$')[0])\n",
    "listname.sort()\n",
    "#for name in listname:\n",
    "#    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Op name: org.esa.snap.core.gpf.common.resample.ResamplingOp\n",
      "Op alias: Resample\n",
      "\n",
      "PARAMETERS:\n",
      "\n",
      "referenceBandName: The name of the reference band. All other bands will be re-sampled to match its size and resolution. Either this or targetResolutionor targetWidth and targetHeight must be set.\n",
      "Default Value: None\n",
      "\n",
      "targetWidth: The width that all bands of the target product shall have. If this is set, targetHeight must be set, too. Either this and targetHeight or referenceBand or targetResolution must be set.\n",
      "Default Value: None\n",
      "\n",
      "targetHeight: The height that all bands of the target product shall have. If this is set, targetWidth must be set, too. Either this and targetWidth or referenceBand or targetResolution must be set.\n",
      "Default Value: None\n",
      "\n",
      "targetResolution: The resolution that all bands of the target product shall have. The same value will be applied to scale image widths and heights. Either this or referenceBand or targetwidth and targetHeight must be set.\n",
      "Default Value: None\n",
      "\n",
      "upsamplingMethod: The method used for interpolation (upsampling to a finer resolution).\n",
      "Default Value: Nearest\n",
      "\n",
      "downsamplingMethod: The method used for aggregation (downsampling to a coarser resolution).\n",
      "Default Value: First\n",
      "\n",
      "flagDownsamplingMethod: The method used for aggregation (downsampling to a coarser resolution) of flags.\n",
      "Default Value: First\n",
      "\n",
      "resampleOnPyramidLevels: This setting will increase performance when viewing the image, but accurate resamplings are only retrieved when zooming in on a pixel.\n",
      "Default Value: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get help of the SNAP operators\n",
    "def Op_help(op):\n",
    "        op_spi = snappy.GPF.getDefaultInstance().getOperatorSpiRegistry().getOperatorSpi(op)\n",
    "        print('Op name: {}'.format(op_spi.getOperatorDescriptor().getName()))\n",
    "        print('Op alias: {}\\n'.format(op_spi.getOperatorDescriptor().getAlias()))\n",
    "        print('PARAMETERS:\\n')\n",
    "        param_Desc = op_spi.getOperatorDescriptor().getParameterDescriptors()\n",
    "        for param in param_Desc:\n",
    "            value_set = param_Desc[0].getValueSet()\n",
    "            if len(value_set) == 0:\n",
    "                print('{}: {}\\nDefault Value: {}\\n'.format(param.getName(),param.getDescription(),param.getDefaultValue()))\n",
    "            else:\n",
    "                print('{}: {}\\nDefault Value: {}\\nPossible param: {}\\n'.format(param.getName(),param.getDescription(),param.getDefaultValue(),list(value_set)))\n",
    "\n",
    "#Op_help(\"Multi-Temporal-Speckle-Filter\")\n",
    "Op_help(\"Resample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and process S1 products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reads directories in a folder containing only S1 products. Then, creates a dictionary using the products names as keys. Then, reads and stores diferent processing steps in a second-level dictionary (i.e. inside the first dictionary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snappy, shutil, os, ast, re\n",
    "from sentinelsat.sentinel import read_geojson, geojson_to_wkt\n",
    "from snappy import ProductIO, HashMap, GPF, jpy\n",
    "\n",
    "GPF.getDefaultInstance().getOperatorSpiRegistry().loadOperatorSpis()\n",
    "HashMap = snappy.jpy.get_type('java.util.HashMap')\n",
    "WKTReader = snappy.jpy.get_type('com.vividsolutions.jts.io.WKTReader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBandNames (product, sfilter = ''):\n",
    "    \"\"\"\n",
    "    Produces a string to use in the sourceBandNames parameter specification of SNAP operators.\n",
    "    Args:\n",
    "        product (): \n",
    "        sfilter (string): regular expression to filter the name of the bands\n",
    "    Output:\n",
    "        returns a string with comma-separated band names\n",
    "    \"\"\"\n",
    "    band_names = product.getBandNames()\n",
    "    if sfilter != '':\n",
    "        band_names = filter(re.compile(r''+sfilter).search, band_names)\n",
    "    if len(band_names) > 0:\n",
    "        band_names = ','.join(band_names)\n",
    "    else:\n",
    "        band_names = None\n",
    "    return band_names\n",
    "\n",
    "def stacking(product_set):\n",
    "    \"\"\"\n",
    "    Takes a list of SNAP products and returns a stacked product with all the bands named with\n",
    "    the products acquisition dates.\n",
    "    Args:\n",
    "        product_set: a list of products to be stacked\n",
    "    Output: returns an individual product with the bands of the other products \n",
    "    \"\"\"\n",
    "    # check if products contain any bands, discard when not\n",
    "    prod_set = [product for product in product_set if not product.getNumBands() == 0]\n",
    "    # define the stack parameters\n",
    "    params = HashMap()\n",
    "    params.put('resamplingType', None)\n",
    "    params.put('initialOffsetMethod', 'Product Geolocation')\n",
    "    params.put('extent', 'Master')\n",
    "    # create the stack\n",
    "    print(\"Creating stack of {} products...\".format(str(len(prod_set)))\n",
    "    create_stack = GPF.createProduct('CreateStack', params, prod_set)\n",
    "    return create_stack\n",
    "\n",
    "def mtspeckle_sigma0 (stacked_prod, pol):\n",
    "    \"\"\"\n",
    "    Applies the a multi-temporal speckle filter to the a corregistered calibrated product stack. Takes the product bands\n",
    "    which name starts with 'Sigma0'.\n",
    "    \n",
    "    Args:\n",
    "        stacked_prod (): product with all the bands to be used for the multi-temporal speckle filter operation\n",
    "        pol (str): polarization to apply the speckle filter (VV or VH)\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    param_specklefilter = HashMap()\n",
    "    param_specklefilter.put('sourceBandNames', getBandNames(stacked_prod, \"Sigma0_\"+pol))\n",
    "    param_specklefilter.put('filter', 'Lee Sigma')\n",
    "    sf_product = GPF.createProduct(\"Multi-Temporal-Speckle-Filter\", param_specklefilter, stacked_prod)\n",
    "    return sf_product\n",
    "\n",
    "def Sigma0_todB (product):\n",
    "    \"\"\"\n",
    "    Transforms the product bands to a logaritmic scale in dB (10*log10[band]).\n",
    "    \n",
    "    Args:\n",
    "        product: product with Sigma0 bands in linear units\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    param_logdB = HashMap()\n",
    "    param_logdB.put('sourceBandNames', getBandNames(product))\n",
    "    db_product = GPF.createProduct(\"LinearToFromdB\", param_logdB, product)\n",
    "    return db_product\n",
    "\n",
    "def write_product (product, out_name):\n",
    "    \"\"\"\n",
    "    Writes a GDF product in BEAM-DIMAP format (.dim). Prints informative text with product name and\n",
    "    names of the bands.\n",
    "    \n",
    "    Args:\n",
    "        product (): product to be written\n",
    "        out_name (str): name/location of the output file\n",
    "    \"\"\"\n",
    "    print('Writing {}, with bands: {}.'.format(out_name, getBandNames(product)))\n",
    "    ProductIO.writeProduct(product, out_name, 'BEAM-DIMAP', pm = createProgressMonitor())\n",
    "\n",
    "def createProgressMonitor():\n",
    "    PWPM = jpy.get_type('com.bc.ceres.core.PrintWriterProgressMonitor')\n",
    "    JavaSystem = jpy.get_type('java.lang.System')\n",
    "    monitor = PWPM(JavaSystem.out)\n",
    "    return monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set processing variables: data location, polarizations and region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a variable for the data location\n",
    "eo_direc = 'D:/eo_data/Saldana/'\n",
    "\n",
    "# Get the names of S1 files in location\n",
    "eo_files = filter(re.compile(r'S1A.*SAFE$').search, os.listdir(eo_direc))\n",
    "\n",
    "## Create a dictionary to read Sentinel-1 L1 GRD products\n",
    "product = {}\n",
    "for element in eo_files:\n",
    "    # second level dictionary to store intermediate products in memory\n",
    "    product[element[:-5]] = {}\n",
    "\n",
    "# Read the area of interest from geoJSON file\n",
    "regPath = \"//dapadfs/workspace_cluster_6/TRANSVERSAL_PROJECTS/MADR/COMPONENTE_2/\" \\\n",
    "    \"Imagenes_Satelitales/Sentinel_2/JSON_Saldana/saldanaextent.geojson\"\n",
    "geom = geojson_to_wkt(read_geojson(regPath))\n",
    "\n",
    "# Define polarizations of interest\n",
    "polarizations = ['VV', 'VH']\n",
    "\n",
    "# Set location for output files\n",
    "pre_outdir = eo_direc + 'prep/'\n",
    "if not os.path.exists(pre_outdir):\n",
    "    os.makedirs(pre_outdir)\n",
    "    print \"New directory '\" + pre_outdir + \"' was created\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply orbit file\n",
    "* Radiometric calibration\n",
    "* Terrain correction\n",
    "* Subset\n",
    "* Write intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading S1A_IW_GRDH_1SSV_20151017T104253\n",
      "Writing D:/eo_data/Saldana/prep/VV_S1A_IW_GRDH_1SSV_20151017T104253, with bands: Sigma0_VV.\n",
      "Writing D:/eo_data/Saldana/prep/VH_S1A_IW_GRDH_1SSV_20151017T104253, with bands: None.\n",
      "Reading S1A_IW_GRDH_1SSV_20160419T231329\n",
      "Writing D:/eo_data/Saldana/prep/VV_S1A_IW_GRDH_1SSV_20160419T231329, with bands: Sigma0_VV.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-55f642c71336>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mparam_calibration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHashMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mparam_calibration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'outputSigmaBand'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mparam_calibration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sourceBands'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetBandNames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'orbit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Intensity_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mpol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mparam_calibration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'selectedPolarisations'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mparam_calibration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'outputImageScaleInDb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for key, value in product.iteritems():    \n",
    "    # Read the product\n",
    "    value['GRD'] = ProductIO.readProduct(eo_direc+key+'.SAFE/manifest.safe')\n",
    "    print('Reading '+key)\n",
    "    \n",
    "    # Apply orbit\n",
    "    param_orbit = HashMap()\n",
    "    value['orbit'] = GPF.createProduct(\"Apply-Orbit-File\", param_orbit, value['GRD'])\n",
    "    \n",
    "    # The following operations are specific por each polarization    \n",
    "    for pol in polarizations:\n",
    "        # Radiometric calibration\n",
    "        param_calibration = HashMap()\n",
    "        param_calibration.put('outputSigmaBand', True)\n",
    "        param_calibration.put('sourceBands', getBandNames(value['orbit'], 'Intensity_'+pol))\n",
    "        param_calibration.put('selectedPolarisations', pol)\n",
    "        param_calibration.put('outputImageScaleInDb', False)\n",
    "        value['calibration_'+pol] = GPF.createProduct(\"Calibration\", param_calibration, value['orbit'])\n",
    "        \n",
    "        # Terrain correction\n",
    "        param_terraincor = HashMap()\n",
    "        param_terraincor.put('demResamplingMethod', 'NEAREST_NEIGHBOUR')\n",
    "        param_terraincor.put('imgResamplingMethod', 'NEAREST_NEIGHBOUR')\n",
    "        param_terraincor.put('applyRadiometricNormalization', True)\n",
    "        param_terraincor.put('demName', 'SRTM 3Sec')\n",
    "        param_terraincor.put('pixelSpacingInMeter', 10.0)\n",
    "        param_terraincor.put('sourceBands', getBandNames(value['calibration_'+pol], 'Sigma0_'+pol))\n",
    "        param_terraincor.put('mapProjection', 'WGS84(DD)')\n",
    "        value['terraincor_'+pol] = GPF.createProduct(\"Terrain-Correction\", param_terraincor, value['calibration_'+pol])\n",
    "        \n",
    "        # Subset to area of interest\n",
    "        param_subset = HashMap()\n",
    "        param_subset.put('geoRegion', geom)\n",
    "        param_subset.put('outputImageScaleInDb', False)\n",
    "        param_subset.put('sourceBandNames', getBandNames(value['terraincor_'+pol], 'Sigma0_'+pol))\n",
    "        value['subset_'+pol] = GPF.createProduct(\"Subset\", param_subset, value['terraincor_'+pol])\n",
    "        \n",
    "        # define the name of the output\n",
    "        output_name = pre_outdir + pol + \"_\" + key\n",
    "        \n",
    "        # Write the results to files\n",
    "        write_product(value['subset_'+pol], output_name)\n",
    "                \n",
    "        # dispose all the intermediate products\n",
    "        value['calibration_'+pol].dispose()\n",
    "        value['terraincor_'+pol].dispose()\n",
    "        value['subset_'+pol].dispose()\n",
    "        \n",
    "    #dispose all the intermediate products\n",
    "    value['GRD'].dispose()\n",
    "    value['orbit'].dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speckle filtering \n",
    "\n",
    "* Read and stack pre-processed products by polarization\n",
    "* Multi-temporal speckle filtering\n",
    "* Scale transformation to dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating stack...\n",
      "Writing , with bands: Sigma0_VV_mst_19Apr2016_db,Sigma0_VV_slv1_02Apr2016_db,Sigma0_VV_slv2_23Aug2015_db,Sigma0_VV_slv3_09Mar2016_db,Sigma0_VV_slv4_30Aug2015_db,Sigma0_VV_slv5_02Mar2016_db,Sigma0_VV_slv6_30Jul2015_db,Sigma0_VV_slv7_06Aug2015_db,Sigma0_VV_slv8_19Jun2015_db,Sigma0_VV_slv9_13Jul2015_db,Sigma0_VV_slv10_10Oct2015_db,Sigma0_VV_slv11_04Dec2015_db,Sigma0_VV_slv12_10Nov2015_db,Sigma0_VV_slv13_03Nov2015_db,Sigma0_VV_slv14_17Oct2015_db,Sigma0_VV_slv15_07Feb2016_db,Sigma0_VV_slv16_21Jan2016_db,Sigma0_VV_slv17_14Feb2016_db,Sigma0_VV_slv18_14Jan2016_db,Sigma0_VV_slv19_21Dec2015_db,Sigma0_VV_slv20_27Nov2015_db,Sigma0_VV_slv21_12Jun2015_db,Sigma0_VV_slv22_16Sep2015_db.\n",
      "Creating stack...\n",
      "Writing , with bands: Sigma0_VH_mst_27Nov2015_db,Sigma0_VH_slv1_16Sep2015_db.\n"
     ]
    }
   ],
   "source": [
    "int_files = os.listdir(eo_direc+'prep/')\n",
    "\n",
    "polarizations = ['VV','VH']\n",
    "\n",
    "## Make stack of polarizations, apply mt speckle filter, log trasnform and write\n",
    "for pol in polarizations:\n",
    "    # filter results by polarization\n",
    "    results = filter(re.compile(r'^'+pol+'.*dim$').search, eo_files)\n",
    "    # declare variable to read products in list\n",
    "    polprods = []\n",
    "    # read product\n",
    "    for result in results:\n",
    "        polprods.append(ProductIO.readProduct(eo_direc+'prep/'+result))\n",
    "    # stack, apply multi-temporal speckle filter and logaritmic transform\n",
    "    stack = Sigma0_todB(mtspeckle_sigma0(stacking(polprods), pol))\n",
    "    # write results\n",
    "    write_product(stack, pre_outdir+pol+'stack_spk_dB')\n",
    "    # memory cleaning\n",
    "    stack.dispose()\n",
    "    polprods = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texture analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramaters for GLCM texture analysis\n",
    "paramGLCM = HashMap()\n",
    "paramGLCM.put('sourceBandNames', 'Sigma0_' + 'VV')\n",
    "paramGLCM.put('windowSizeStr', '5x5')\n",
    "paramGLCM.put('quantizerStr', 'Probabilistic Quantizer')\n",
    "paramGLCM.put('quantizationLevelsStr', '16')\n",
    "paramGLCM.put('displacement','4' )\n",
    "paramGLCM.put('outputContrast','true')\n",
    "paramGLCM.put('outputDissimilarity','true')\n",
    "paramGLCM.put('outputHomogeneity','true')\n",
    "paramGLCM.put('outputASM','true')\n",
    "paramGLCM.put('outputEnergy','true')\n",
    "paramGLCM.put('outputMean','true')\n",
    "paramGLCM.put('outputVariance','true')\n",
    "paramGLCM.put('outputCorrelation','true')\n",
    "product = GPF.createProduct(\"GLCM\", paramGLCM, product)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
